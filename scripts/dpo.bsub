## Scheduler parameters ##

#BSUB -J dpo_llama                     # job name
#BSUB -o %J.stdout                  # optional: have output written to specific file
#BSUB -e %J.stderr                  # optional: have errors written to specific file
# #BSUB -q batch_a100               # optional: use highend nodes w/ Volta GPUs (default: Geforce GPUs)
#BSUB -W 168:00                      # fill in desired wallclock time [hours,]minutes (hours are optional)
#BSUB -n 1                          # min CPU cores,max CPU cores (max cores is optional)
#BSUB -R "span[hosts=1]"          # optional: run on single host (if using more than 1 CPU core)
#BSUB -M 40960                       # fill in required amount of memory per CPU core(in Mbyte)
#BSUB -P dpo_llama                   # optional: fill in cluster project
#BSUB -gpu "num=8:mode=exclusive_process:mps=no:aff=no"

# Here comes your code
# 1. load some module
module purge
module load conda proxy4server-access cuda/11.8.0 cudnn/11.8_v8.9
# proxy_on
source /fs/applications/p4s-access/2.0/ActivateP4S.sh -a

# 2. define some variables
## environment
vEnv=lavine_clf
## scripts
proj_path=/fs/scratch/rng_cr_bcai_dl/law1rng/lavine_bosch/lavine_code/RLCLF/DPO
dpo_output_dir=$proj_path/dpo_llama
model_name_or_path=/fs/scratch/rng_cr_bcai_dl/law1rng/lavine_bosch/lavine_code/RLCLF/SFT/sft_10k_llama_chat/final_merge


# 3. load enviroment
conda activate $vEnv
mkdir $dpo_output_dir

# 4. run your code here
nohup accelerate launch --config_file $proj_path/acc_config.yaml $proj_path/dpo_training.py \
--model_name_or_path=$model_name_or_path \
--per_device_train_batch_size 8 --per_device_eval_batch_size 8 \
--max_steps 60000 --save_steps 100 \
--output_dir $dpo_output_dir \
> $dpo_output_dir/log.txt &
